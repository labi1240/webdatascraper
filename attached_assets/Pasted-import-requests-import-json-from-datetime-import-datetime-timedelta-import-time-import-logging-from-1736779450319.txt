import requests
import json
from datetime import datetime, timedelta
import time
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor, as_completed

# Configure logging
logging.basicConfig(
    level=logging.INFO,  # Change to DEBUG for detailed logs during debugging
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraping.log"),
        logging.StreamHandler()
    ]
)

# Define cookies (ensure unique keys)
cookies = {
    '_ga': 'GA1.3.6299276.1729021923',
    '_ga_1': 'GA1.1.6299276.1729021923',  # Renamed to avoid duplication
    '_gid': 'GA1.3.1792005162.1729112353',
    '_gat': '1',
    '_gat_1': '1',
    's': 'eyJwYXNzcG9ydCI6eyJ1c2VyIjoiNTM3ZmE2ZWU1ZTU1OTlhODU1ZDJmODFiIn0sInMiOiJhYWQ5YmY5Zi03YmEzLTQxZGUtYTFjZS1jNGZkMWFjM2IyZWEiLCJub3ciOjI4ODE5OTQzLCJpIjo4MH0=',
    's.sig': 'ASpJMzN-iA35W9KNclYWvG1ZwBs',
    '_ga_G0XQP73LHV': 'GS1.1.1729193701.4.1.1729196603.21.1.1860857804',
}

# Define headers
headers = {
    'accept': 'application/json',
    'accept-language': 'en-US,en;q=0.9',
    # 'authorization': 'Bearer null',  # Remove or replace with a valid token if required
    'baggage': 'sentry-environment=production,sentry-public_key=e9f1abedeae34f04a098c5c0ebb1737a,sentry-trace_id=4d885e178ee6450ab482343ccdd4e648,sentry-sample_rate=0.1,sentry-sampled=false',
    'referer': 'https://app.realmmlp.ca/s?%24orderby=daysOnMarket+desc&is_map_search=true&offset=1&q=treb%2Favailability%3AU%257CgeoAnd%3AY%257CunavailableDate%3A%253E%253D01%252F1%252F2024%257Carea%3APeel%257Cstatus%3ATER%257Cdistrict%3ABrampton%2CMississauga%257Cclass%3AFREE%257CsaleOrRent%3ASALE%257ClastUpdateDate%3A%253E%253D10%252F03%252F2024%2C%253C%253D10%252F10%252F2024',
    'sec-ch-ua': '"Google Chrome";v="129", "Not=A?Brand";v="8", "Chromium";v="129"',
    'sec-ch-ua-mobile': '?1',
    'sec-ch-ua-platform': '"Android"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'sentry-trace': '4d885e178ee6450ab482343ccdd4e648-ac8d50b4649cdddd-0',
    'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Mobile Safari/537.36',
    'x-auth-userid': '537fa6ee5e5599a855d2f81b',
    'x-requested-with': 'XMLHttpRequest',
}

# Define base parameters without conflicting pagination parameters
base_params = {
    'availability': 'U',
    'geoAnd': 'Y',
    'unavailableDate': '>=01/1/2024',
    'area': 'Peel',
    'status': 'TER',
    'district[0]': 'Brampton',
    'district[1]': 'Mississauga',
    'class': 'FREE',
    'saleOrRent': 'SALE',
    '$gid': 'treb',
    'gid': 'TREB',
    '$isMapSearch': 'true',
    '$meta[isMapSearch]': 'true',
    '$orderby': 'daysOnMarket desc',
    '$output': 'list',
    '$select[0]': 'streetAddress',
    '$select[1]': 'city',
    '$select[2]': 'bedrooms',
    '$select[3]': 'bedroomsPossible',
    '$select[4]': 'bathrooms',
    '$select[5]': 'typeName',
    '$select[6]': 'style',
    '$select[7]': 'price',
    '$select[8]': 'price',
    '$select[9]': 'squareFeetText',
    '$select[10]': 'squareFeet',
    '$select[11]': 'status',
    '$select[12]': 'daysOnMarket',
    '$select[13]': 'listingID',
    '$imageSizes[0]': '150',
    '$imageSizes[1]': '600',
    '$project': 'summary',
    '$skip': '0',    # Starting point; will be updated dynamically
    '$take': '200',  # Number of records to retrieve per request
    # '$pageSize': '20',  # Removed to avoid conflicts
}

def create_session():
    """
    Create a new session with retry strategy.
    This function is called by each thread to have its own session.
    """
    session = requests.Session()
    retry = Retry(
        total=5,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"],
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('https://', adapter)
    session.mount('http://', adapter)
    session.headers.update(headers)
    session.cookies.update(cookies)
    return session

def fetch_results(skip, take, last_update_start, last_update_end, session):
    """
    Fetch a batch of listings from the API based on the provided date range.

    :param skip: Number of records to skip (for pagination).
    :param take: Number of records to retrieve.
    :param last_update_start: Start date for filtering listings.
    :param last_update_end: End date for filtering listings.
    :param session: The requests session to use for the HTTP request.
    :return: List of listings or an empty list if none are found.
    """
    params = base_params.copy()
    params['$skip'] = str(skip)
    params['$take'] = str(take)
    params['lastUpdateDate[0]'] = f">={last_update_start}"
    params['lastUpdateDate[1]'] = f"<={last_update_end}"

    try:
        response = session.get(
            'https://app.realmmlp.ca/search',
            params=params,
            timeout=10
        )
        response.raise_for_status()
        data = response.json()
        logging.debug(f"Response Data: {json.dumps(data, indent=4)}")

        # Correctly access the 'data' list within 'searchResults'
        listings = data.get('searchResults', {}).get('data', [])
        return listings
    except requests.exceptions.HTTPError as http_err:
        logging.error(f"HTTP error occurred: {http_err}")
        logging.error(f"Response content: {response.text}")
    except requests.exceptions.RequestException as req_err:
        logging.error(f"Request exception: {req_err}")
    except ValueError:
        logging.error("Error decoding JSON response.")
        logging.error(f"Response content: {response.text}")
    return []

def fetch_all_pages_for_date_range(formatted_date_start, formatted_date_end):
    """
    Fetch all pages of listings for a given date range.

    :param formatted_date_start: Start date in MM/DD/YYYY format.
    :param formatted_date_end: End date in MM/DD/YYYY format.
    :return: List of listings for the date range.
    """
    listings = []
    skip = 0
    take = 200
    more_data = True
    session = create_session()  # Each thread uses its own session
    while more_data:
        batch = fetch_results(
            skip=skip,
            take=take,
            last_update_start=formatted_date_start,
            last_update_end=formatted_date_end,
            session=session
        )
        if batch:
            listings.extend(batch)
            skip += take
            if len(batch) < take:
                more_data = False
        else:
            more_data = False
    return listings

def paginate_results(start_date, end_date, delta=timedelta(days=4)):
    """
    Retrieve all listings by paginating through the API based on specified date ranges.

    :param start_date: The most recent date to start fetching listings.
    :param end_date: The oldest date to stop fetching listings.
    :param delta: The time delta to decrement each iteration (default is 4 days).
    :return: List of all fetched listings.
    """
    all_results = []
    date_ranges = []

    current_end_date = start_date
    current_start_date = start_date - delta
    max_iterations = 1000  # Prevent infinite loops; adjust as needed
    iteration = 0

    while current_start_date >= end_date and iteration < max_iterations:
        formatted_date_start = current_start_date.strftime('%m/%d/%Y')
        formatted_date_end = current_end_date.strftime('%m/%d/%Y')
        date_ranges.append((formatted_date_start, formatted_date_end))

        current_end_date = current_start_date - timedelta(days=1)
        current_start_date = current_end_date - delta
        iteration += 1

    logging.info(f"Total date ranges to process: {len(date_ranges)}")

    max_workers = 3  # Adjust based on your system's capability
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_all_pages_for_date_range, dr[0], dr[1]): dr for dr in date_ranges}

        for future in as_completed(futures):
            date_range = futures[future]
            try:
                listings = future.result()
                if listings:
                    all_results.extend(listings)
                    logging.info(f"Fetched {len(listings)} listings for date range {date_range}. Total so far: {len(all_results)}")
                else:
                    logging.info(f"No listings found for date range {date_range}.")
            except Exception as e:
                logging.error(f"Error fetching data for date range {date_range}: {e}")

    logging.info(f"Total listings fetched: {len(all_results)}")
    return all_results

def save_results(results, filename='mylistings.json'):
    """
    Save the fetched listings to a JSON file, ensuring no duplicates.

    :param results: List of listings to save.
    :param filename: The filename for the saved JSON data.
    """
    unique_results = {listing['listingID']: listing for listing in results if 'listingID' in listing}
    try:
        with open(filename, 'w') as f:
            json.dump(list(unique_results.values()), f, indent=4)
        logging.info(f"Results successfully saved to {filename}")
    except IOError as e:
        logging.error(f"Failed to save results to {filename}: {e}")

if __name__ == "__main__":
    # Define the start and end dates
    # Example: Fetch listings updated from 01/01/2024 to 10/17/2024
    end_date_str = '01/01/2025'    # MM/DD/YYYY
    start_date_str = '01/07/2025'  # MM/DD/YYYY (today's date)

    # Convert string dates to datetime objects
    try:
        start_date = datetime.strptime(start_date_str, '%m/%d/%Y')
        end_date = datetime.strptime(end_date_str, '%m/%d/%Y')
    except ValueError as ve:
        logging.error(f"Date format error: {ve}")
        exit(1)

    # Fetch all listings within the date range
    all_data = paginate_results(start_date=start_date, end_date=end_date)

    # Save the results to a JSON file
    if all_data:
        save_results(all_data)
    else:
        logging.info("No data fetched to save.")
